[{"pk": 1, "model": "blog.post", "fields": {"description": "Originally authored by: http://www.apreche.net/", "created": "2013-05-19T14:49:28Z", "title": "Complete Single Server Django Stack Tutorial", "content": "<h4>Introduction</h4>\r\n<p>I notice a lot of people these days using <a href=\"http://www.heroku.com/\">Heroku</a> to host applications they&#8217;ve written with <a href=\"https://www.djangoproject.com/\">Django</a> or <a href=\"http://rubyonrails.org/\">Rails</a>. This may save them some time and effort, but they are spending way more money on this specialized hosting than they would pay for a generic Linux host. They are also giving up control of a large portion of their application. I consider the entire stack to be part of my application, and complete control over all the pieces is mandatory.</p>\r\n<p>Because I encourage people to administer their own servers, I have written this tutorial to remove just one more of their excuses not to do so. This tutorial covers everything you need to do to configure a blank <a href=\"http://releases.ubuntu.com/precise/\">Ubuntu Linux 12.04 LTS 64-bit server</a> to run a Django application properly. I will not be covering anything about writing the application itself. I will also only provide minimal discussion of some topics, like database configuration. If you can finish this tutorial, you can Google the necessary documentation for other things. The only thing I completely skip is configuration of search using <a title=\"Haystack\" href=\"http://haystacksearch.org/\">Haystack</a> and <a href=\"http://lucene.apache.org/solr/\">Solr</a> since it is not needed by most applications.</p>\r\n<p>If you want to use a different Linux distribution, most of this tutorial will still be correct. You will have to translate all of the apt-get statements and package names to your distribution of choice. Configuration file paths and layouts may also differ. I chose Ubuntu 12.04 because it is very popular, available on almost every host, and its long term support will keep this tutorial relevant for a longer period of time.</p>\r\n<h4><span id=\"more-1339\"></span>Formatting</h4>\r\n<p>In this tutorial all commands and lines of code that you should actually enter are displayed in preformatted text. Within the preformatted text there will be lines that begin with an octothorpe (#). Those lines contain my personal comments on the surrounding lines, and should not be typed. Lines beginning with the dollar symbol ($) are commands that should be typed on the shell. In cases where there are variables that you should replace with appropriate values, I will surround the variable with &lt;&gt; symbols and name it with all capital letters.</p>\r\n<pre># This text is describing the command below it. Don't type it.\r\n$ this is a shell command, type it\r\n\r\n# Type the following command, but replace the &lt;USER&gt; with your username\r\n$ sudo adduser &lt;USER&gt;</pre>\r\n<p>In cases where edits must be made to a file I will begin that section with a shell command to open the appropriate file in vim, which is my text editor of choice. You may use whichever text editor you wish. I will also describe the edits to be made in the comment text.</p>\r\n<pre>$ vim test.txt\r\n\r\n# add this line to the end of the file\r\nTEST = True</pre>\r\n<h4>Tutorial</h4>\r\n<h4>Beginning Steps</h4>\r\n<p>Begin by installing Ubuntu on a machine you own, or getting a new machine from a hosting provider. I recommend <a href=\"http://www.linode.com/\">Linode</a> or <a href=\"http://aws.amazon.com/ec2/\">Amazon EC2</a>. If you choose EC2 the <a href=\"http://cloud-images.ubuntu.com/locator/ec2/\">Ubuntu AMI finder</a> should be your starting point.</p>\r\n<p>Once the machine is installed, you will need to login, mostly likely using <a href=\"http://www.openssh.com/\">SSH</a>. I can not more strongly recommend you use <a href=\"http://www.unixwiz.net/techtips/ssh-agent-forwarding.html\">SSH Agent Forwarding</a>. I will not cover how to set it up in this tutorial, as it varies based on your client machine. If you haven&#8217;t done this already, do it now.</p>\r\n<p>Once you are logged into the machine, you will probably notice it is telling you that a lot of packages need updating. You should update packages whenever they need updating, especially if there are security patches. Just be aware this may disrupt your site for a few minutes. If your site were important enough to where a minute of downtime was a problem, you would not need this tutorial. Here is how you update all packages on an Ubuntu or Debian system.</p>\r\n<pre># update the local package index\r\n$ sudo apt-get update\r\n\r\n# actually upgrade all packages that can be upgraded\r\n$ sudo apt-get dist-upgrade\r\n\r\n# remove any packages that are no longer needed\r\n$ sudo apt-get autoremove\r\n\r\n# reboot the machine, which is only necessary for some updates\r\n$ sudo reboot</pre>\r\n<p>You may notice frequent use of sudo. Sudo is a program that allows us to execute other commands with root privileges. If you are bothered having to type your password when using sudo, then you can <a href=\"http://askubuntu.com/questions/135821/can-i-run-sudo-without-entering-a-password\">edit your sudoers</a> file to make it easier in the future.</p>\r\n<h4>distribute and pip</h4>\r\n<p>Many of the Python packages we need are available through Ubuntu and apt-get. However, we do not want to use them. The packages provided by Ubuntu are very likely to be old and out of date. Also, it is only possible to install them globally, whereas we would like to install them in an isolated fashion. If we ran two sites on this server that used different versions of Django, this method would not work.</p>\r\n<p>Many of the Python packages are wrappers around C libraries that will need to be available for linking. Those C libraries must be installed using apt-get before we install the associated Python packages. There are two such packages that are needed for just about everything, and should be installed now. They are build-essential and python-dev. Build-essential is the Ubuntu package which provides all the standard C compilation tools. Python-dev provides the necessary files for compiling Python/C modules.</p>\r\n<pre>$ sudo apt-get install build-essential python-dev</pre>\r\n<p>Now, there are various tools to install Python packages, and there is much drama and debate in the community about them. I will ignore all that, and say to use <a href=\"http://packages.python.org/distribute/\">Distribute</a> and <a href=\"http://www.pip-installer.org/en/latest/\">pip</a>. I have used them for years now, and have had no issues at all.</p>\r\n<pre># download distribute\r\n$ curl -O http://python-distribute.org/distribute_setup.py\r\n\r\n# install distribute\r\n$ sudo python distribute_setup.py\r\n\r\n# remove installation files\r\n$ rm distribute*\r\n\r\n# use distribute to install pip\r\n$ sudo easy_install pip</pre>\r\n<h4>Virtualenv(wrapper)</h4>\r\n<p>Now that we have used distribute to install pip, we will use pip to install every other Python package. This solves the problem of getting packages, but it does not solve the problem of isolating them. For that we must use <a href=\"http://www.virtualenv.org/en/latest/\">virtualenv</a>. Because virtualenv itself is a little clumsy, we will also use <a href=\"http://www.doughellmann.com/projects/virtualenvwrapper/\">virtualenvwrapper</a> to make things a little easier on ourselves.</p>\r\n<pre># install virtualenv and virtualenvwrapper\r\n$ sudo pip install virtualenv virtualenvwrapper\r\n\r\n# edit the .bashrc file\r\n$ vim .bashrc\r\n\r\n# to enable virtualenvwrapper add this line to the end of the file\r\nsource /usr/local/bin/virtualenvwrapper.sh\r\n#save and quit your editor\r\n\r\n# exit and log back in to restart your shell\r\n$ exit</pre>\r\n<p>You will notice after exiting and logging back into the shell that virtualenvwrapper will do a bunch of magic. That is a one-time setup that you will never see again. If you don&#8217;t see a bunch of stuff happen, then there is a problem you should fix.</p>\r\n<p>Now, how do you use virtualenv? You create as many virtualenvs as you like, each with a unique name. Only one of these virtualenvs will be active at a given time. Any time you use pip the packages will only be installed into the active virtualenv. In order for a Python program to make use of those Python packages, it must be executed with that virtualenv activated. Let&#8217;s create a virtualenv for our project and activate it. You should just keep it active for the rest of this tutorial for ease. You can tell when a virtualenv is active because it will appear in your shell prompt.</p>\r\n<pre># create a virtualenv, I usually give it the same name as my app\r\n$ mkvirtualenv &lt;VIRTUALENV_NAME&gt;\r\n\r\n# The virtualenv will be activated automatically.\r\n# You can deactivate it like this\r\n$ deactivate\r\n\r\n# to activate a virtualenv, or change which one is active, do this\r\n$ workon &lt;VIRTUALENV_NAME&gt;</pre>\r\n<p>One very helpful command you can use now is pip freeze. Pip freeze lists all available Python packages and their versions. It will let you easily see what has been installed. Try these commands out. You will see just how many packages have already been installed system-wide by Ubuntu. You will also see that your virtualenv is isolated from the rest of the system, and doesn&#8217;t have much installed in it yet.</p>\r\n<pre>$ deactivate\r\n$ pip freeze\r\n$ workon &lt;VIRTUALENV_NAME&gt;\r\n$ pip freeze</pre>\r\n<h4>Django</h4>\r\n<p>Installing Django is very simple. Make sure the virtualenv is activated. This is the last time I will remind you to activate.</p>\r\n<pre># install the newest version of Django\r\n$ pip install django\r\n\r\n# also install docutils, which is used by the django admin\r\n$ pip install docutils\r\n\r\n# If you want to test django, start a new project and run it\r\n$ django-admin.py startproject &lt;APP_NAME&gt;\r\n$ cd &lt;APP_NAME&gt;\r\n# make manage.py executable\r\n$ chmod +x manage.py\r\n# start the dev server\r\n$ ./manage.py runserver 0.0.0.0:8000</pre>\r\n<h4>PIL / Pillow</h4>\r\n<p><a href=\"http://www.pythonware.com/products/pil/\">PIL</a> is the Python imaging library. It is needed if your application is going to do anything involving at all involving images. Even if you simply allow user to upload images, you will need it. There is a drop in replacement available called <a href=\"http://pypi.python.org/pypi/Pillow/\">Pillow</a>. I&#8217;ve never had problems with either one, but some people have problems with PIL. If you don&#8217;t know, just choose Pillow. Whichever one you choose, you will need the same set of prerequisite C libraries. If you don&#8217;t install these, then PIL might fail if a user tries to upload a jpg, or you try to perform certain image operations.</p>\r\n<pre># install libraries\r\n$ sudo apt-get install libjpeg8-dev libfreetype6-dev zlib1g-dev\r\n\r\n# choose ONE of these\r\n$ pip install pillow\r\n$ pip install pil</pre>\r\n<h4>Databases MySQL / PostgreSQL</h4>\r\n<p>It&#8217;s up to you to choose your database. I prefer <a href=\"http://www.mysql.com/\">MySQL</a> because it is easier. The Django devs prefer <a href=\"http://www.postgresql.org/\">PostgreSQL</a> because it is more robust. You can learn to configure the databases using their respective documentation. It&#8217;s also up to you to edit the Django settings.py file to point at the appropriate database server. If you choose MySQL, you should configure it so that utf8 is always the default character set, utf8_general_ci is the default collation, and that InnoDB is the default storage engine.</p>\r\n<pre># do this to go with Postgres\r\n$ sudo apt-get install postgresql libpq-dev\r\n$ pip install psycopg2\r\n\r\n# do this to go with MySQL\r\n$ sudo apt-get install mysql-server libmysqlclient-dev\r\n$ pip install MySQL-Python</pre>\r\n<p>Whichever database you choose, you will have to create a database and a user for your application to use. Then you will have to make sure that the user has full permissions on that database.</p>\r\n<h4>South</h4>\r\n<p><a href=\"http://south.aeracode.org/\">South</a> is a mandatory part of every Django project as far as I am concerned. It manages schema changes to your database about as well as can be expected, and makes life much easier in general.</p>\r\n<pre>$ pip install south\r\n\r\n# add south to the INSTALLED_APPS in your settings.py\r\n$ vim &lt;YOUR_APP&gt;/settings.py\r\nINSTALLED_APPS = \r\n    ...\r\n    'south',\r\n    ...</pre>\r\n<h4>memcached</h4>\r\n<p>Just about every web site can benefit from having memcached. It creates a key:value store directly in memory that is very fast and easy. This can be used to store information to reduce database queries and increase performance. There have been many different libraries and modules for memcached, but PyLibMC is presently the preferred choice.</p>\r\n<pre># install memcached server and library\r\n$ sudo apt-get install memcached libmemcached-dev\r\n\r\n# install pylibmc\r\n$ pip install pylibmc\r\n\r\n# edit the CACHES setting in your project's settings.py\r\n$ vim &lt;YOUR_APP&gt;/settings.py\r\nCACHES = {\r\n    'default': {\r\n        'BACKEND': 'django.core.cache.backends.memcached.PyLibMCCache',\r\n        'LOCATION': '127.0.0.1:11211',\r\n    }\r\n}</pre>\r\n<h4>Asynchronous Task Execution</h4>\r\n<p>Almost every site can benefit from asynchronous task execution. Even if your site isn&#8217;t big enough to make it a necessity for scaling and performance, why not do it? It&#8217;s very easy to set it up, and requires only free software.</p>\r\n<p>What is asynchronous task execution? Let&#8217;s say you have a simple contact form. If you call send_email from the contact form view, then the user who submitted the form will not receive an HTTP response until the email has been sent. Their browser will sit there spinning while the server works. It will also tie up one of your web server threads. To make matters worse, if there is an email error, it will return that error to the user.</p>\r\n<p>What you really want to do is send the user to a thank you page immediately after they submit. Then you can send the email some time later without tying up a web server. If there is an error, you can keep retrying it without bothering the user. To do this we use a Python package called <a href=\"http://celeryproject.org/\">celery</a>.</p>\r\n<p>Celery requires a few pieces to work. First, it needs celery workers. These are the programs that actually do the work, such as sending the emails. Next it needs a message queuing server. This is where the celery workers look to see if there is any work they should be doing. For this you should use <a href=\"http://www.rabbitmq.com/\">RabbitMQ</a>.</p>\r\n<p>There is also celerycam, which will monitor celery by taking snapshots every few seconds. This will allow you to see the state of what celery is doing from the Django admin interface. Lastly, we need a place to store results if any celery tasks produce them. For that we will just use the existing MySQL or PostgreSQL database.</p>\r\n<h4>RabbitMQ</h4>\r\n<p>When you install RabbitMQ you also have to create a user and grant them permissions on a virtual host. You can pick whatever username, password, and vhost name you like, but I always use the name of my app for all three.</p>\r\n<pre>$ sudo apt-get install rabbitmq-server\r\n$ sudo rabbitmqctl add_user &lt;RABBIT_USER&gt; &lt;RABBIT_PASSWORD&gt;\r\n$ sudo rabbitmqctl add_vhost &lt;RABBIT_VHOST&gt;\r\n$ sudo rabbitmqctl set_permissions -p &lt;RABBIT_VHOST&gt; &lt;RABBIT_USER&gt; \".*\" \".*\" \".*\"</pre>\r\n<h4>Celery</h4>\r\n<p>Installing celery itself is a one liner, but it requires a lot of modifications to the Django settings file to get it working properly.</p>\r\n<pre>$ pip install django-celery\r\n\r\n$ vim &lt;YOUR_APP&gt;/settings.py\r\n\r\n# add djcelery to INSTALLED_APPS\r\nINSTALLED_APPS = \r\n    ...\r\n    'djcelery',\r\n    ...\r\n\r\n# add these settings\r\nBROKER_URL = \"amqp://&lt;RABBIT_USER&gt;:&lt;RABBIT_PASSWORD&gt;@localhost:5672//&lt;RABBIT_VHOST&gt;\"\r\nCELERY_RESULT_BACKEND = \"database\"\r\n\r\n# choose the setting that matches your database of choice\r\nCELERY_RESULT_DBURI = \"mysql://&lt;DB_USER&gt;:&lt;DB_PASSWORD&gt;@localhost/&lt;DB_NAME&gt;\"\r\nCELERY_RESULT_DBURI = \"postgresql://&lt;DB_USER&gt;:&lt;DB_PASSWORD&gt;@localhost/&lt;DB_NAME&gt;\"\r\n\r\n# put these two lines at the very bottom of the settings file\r\nimport djcelery\r\ndjcelery.setup_loader()</pre>\r\n<p>To test celery you can start up the celery daemon using a management command. If your application is going to have recurring tasks, you should enable events and celerybeat. Otherwise, you can ignore those options.</p>\r\n<pre># start celery with Beat and Event\r\n$ ./manage.py celeryd -B -E\r\n\r\n# start celery normally\r\n$ ./manage.py celeryd\r\n\r\n# press Ctrl+C to quit celery once you see it is working</pre>\r\n<h4>gunicorn</h4>\r\n<p>When you want to serve a modern Python web application, you use a thing called the <a href=\"http://en.wikipedia.org/wiki/Web_Server_Gateway_Interface\">web server gateway interface</a> (WSGI). That is how your application will talk to the web server. There are a lot of choices when it comes to using WSGI. <a href=\"http://httpd.apache.org/\">Apache</a> with <a href=\"http://code.google.com/p/modwsgi/\">mod_wsgi</a> is the old trustworthy option. <a href=\"http://projects.unbit.it/uwsgi/\">uWSGI</a> is also very popular. I personally prefer <a href=\"http://gunicorn.org/\">Gunicorn</a> because it works extremely well, and runs great out of the box without any configuration.</p>\r\n<pre>$ pip install gunicorn\r\n\r\n# add gunicorn to INSTALLED_APPS\r\n$ vim &lt;YOUR_APP&gt;/settings.py\r\nINSTALLED_APPS =\r\n    ...\r\n    'gunicorn',\r\n    ...</pre>\r\n<p>Like celery, you can test Gunicorn by running it from a management command. The Gunicorn management command has many command line parameters. You can read about the options in the <a href=\"http://docs.gunicorn.org/en/latest/configure.html\">Gunicorn documentation</a>. Personally, the only options I use are to set Gunicorn to use four workers and to enable <a href=\"http://www.gevent.org/\">gevent</a>. You will see more about this in the next section.</p>\r\n<pre>$ ./manage.py run_gunicorn -w 4 -k gevent\r\n\r\n# pres Ctrl+C to exit Gunicorn once you see it is working</pre>\r\n<h4>Supervisor</h4>\r\n<p>You may have noticed by now that Ubuntu is very clever in the way it handles servers. When you install MySQL with apt-get it immediately starts it with a default configuration. The same goes for RabbitMQ, memcached, SSH, and almost every other such server. If the machine reboots, these services will start themselves automatically. We can trust the distribution to take care of this for us.</p>\r\n<p>But we have a problem now. We need to start Gunicorn and celery as services, but Ubuntu won&#8217;t help us with things we didn&#8217;t install with apt-get. One way to solve this problem is to write upstart or init.d scripts for these services. That will work, but it&#8217;s a pain in the ass. Those kinds of scripts are not simple to write, and are worse to maintain. Luckily <a href=\"http://supervisord.org/\">supervisor</a> exists.</p>\r\n<p>Supervisor is installed with apt-get, so it will start automatically. We give supervisor very simple configuration files for any further services we would like it to manage, and it will start those up for us. It has the added benefit that it can restart any service it is managing in the case of failure. If Gunicorn crashes, it will come right back without us having to do anything. Because of this feature, some people choose to manage all their services under supervisor. I do not do this because it is extra work, and it is very rare that it will help you. If something like MySQL crashes, odds are it will not be successfully or safely restarted by supervisor.</p>\r\n<p>Installing supervisor is as simple as apt-get, but configuring it is another matter. We need to create a configuration file for each service that supervisor manages. These configuration files must be located in /etc/supervisor/conf.d/ and must have a name with the conf extension. I have included here the full contents of my three supervisor configuration files, but have only documented the first one, as they are so similar.</p>\r\n<pre>$ sudo apt-get install supervisor</pre>\r\n<pre># contents of /etc/supervisor/conf.d/celeryd.conf\r\n\r\n# the name of this service as far as supervisor is concerned\r\n[program:celeryd]\r\n\r\n# the command to start celery\r\ncommand = /home/&lt;USERNAME&gt;/.virtualenvs/&lt;VIRTUALENV_NAME&gt;/bin/python /home/&lt;USERNAME&gt;/&lt;APP_NAME&gt;/manage.py celeryd -B -E\r\n\r\n# the directory to be in while running this\r\ndirectory = /home/&lt;USERNAME&gt;/&lt;APP_NAME&gt;\r\n\r\n# the user to run this service as\r\nuser = &lt;USERNAME&gt;\r\n\r\n# start this at boot, and restart it if it fails\r\nautostart = true\r\nautorestart = true\r\n\r\n# take stdout and stderr of celery and write to these log files\r\nstdout_logfile = /var/log/supervisor/celeryd.log\r\nstderr_logfile = /var/log/supervisor/celeryd_err.log</pre>\r\n<pre># contents of /etc/supervisor/conf.d/celerycam.conf\r\n[program:celerycam]\r\ncommand = /home/&lt;USERNAME&gt;/.virtualenvs/&lt;VIRTUALENV_NAME&gt;/bin/python /home/&lt;USERNAME&gt;/&lt;APP_NAME&gt;/manage.py celerycam\r\ndirectory = /home/&lt;USERNAME&gt;/&lt;APP_NAME&gt;\r\nuser = &lt;USERNAME&gt;\r\nautostart = true\r\nautorestart = true\r\nstdout_logfile = /var/log/supervisor/celerycam.log\r\nstderr_logfile = /var/log/supervisor/celerycam_err.log</pre>\r\n<pre># contents of /etc/supervisor/conf.d/gunicorn.conf\r\n[program:gunicorn]\r\ncommand = /home/&lt;USERNAME&gt;/.virtualenvs/&lt;VIRTUALENV_NAME&gt;/bin/python /home/&lt;USERNAME&gt;/&lt;APP_NAME&gt;/manage.py run_gunicorn -w 4 -k gevent\r\ndirectory = /home/&lt;USERNAME&gt;/&lt;APP_NAME&gt;\r\nuser = &lt;USERNAME&gt;\r\nautostart = true\r\nautorestart = true\r\nstdout_logfile = /var/log/supervisor/gunicorn.log\r\nstderr_logfile = /var/log/supervisor/gunicorn_err.log</pre>\r\n<p>You will notice the command for each of the supervisor configurations is strange. Instead of just running manage.py directly, we are specifically calling the python interpreter located inside of our virtualenv. There is no way for supervisor to easily activate itself the way we activate in our shell with the workon command. By specifically using the python interpreter from the virtualenv instead of the system version, celery and gunicorn will have the correct path settings, and will use the packages within the the virtualenv. Also, note the celeryd and gunicorn command line parameters we had discussed earlier to enable beat, event, gevent, etc.</p>\r\n<p>To make supervisor recognize the new configuration files, we must restart it. It&#8217;s also pretty obvious from the example blow how to manually control the services that supervisor manages.</p>\r\n<pre>\r\n# Make supervisor aware of new/changed configuration files\r\n$ sudo supervisorctl reread\r\n\r\n# Update and start supervisor processes\r\n$ sudo supervisorctl update\r\n \r\n# restart supervisor itself\r\n$ sudo service supervisor restart\r\n\r\n# restart/stop/start all services managed by supervisor\r\n$ sudo supervisorctl restart all\r\n$ sudo supervisorctl stop all\r\n$ sudo supervisorctl start all\r\n\r\n# restart just celeryd\r\n$ sudo supervisorctl restart celeryd\r\n\r\n# start just gunicorn\r\n$ sudo supervisorctl start gunicorn</pre>\r\n<h4>Static and Media</h4>\r\n<p>There is a great deal of confusion about static and media in Django. It is partially because the separation of the two was not very clear in earlier releases. This has since been fixed, and the static files system has been completely reworked. While the problem is solved in  the current versions, the big changes may have added to the confusion. Allow me to help.</p>\r\n<p>Static files are files that you would put into your repository that are not code. These are almost always css, jss, and image files. Your favicon would also be a static file. This is a file that you make, must be served to web browsers, but does not change unless you update your application.</p>\r\n<p>Media files are files that are also served statically, but they were not made by you. They were uploaded by users. So on a site like YouTube, the YouTube logo in the top left would be a static file, but the actual videos themselves would be media. On Flickr, the css and the login buttons would be static, but the photos uploaded by users are media.</p>\r\n<p>For media files we must simply take files that users upload, put them in a folder, and then serve the files in that folder. For static files we have a slightly trickier issue because they are not so easily located. The django admin keeps its static files in one place. You put your static files in another place. Maybe you pip install another third party module that includes some static files. In this case, you run a management command to collect the static files to a single folder from which they will be served.</p>\r\n<pre># Create directories to hold static and media\r\nsudo mkdir /var/www\r\nsudo mkdir /var/www/static\r\nsudo mkdir /var/www/media\r\n\r\n# set permissions on these directories\r\nsudo chown -R &lt;USERNAME&gt;:www-data /var/www\r\n\r\n$ vim &lt;YOUR_APP&gt;/settings.py\r\n\r\n# these settings are explained below\r\nMEDIA_ROOT = '/var/www/media/'\r\nMEDIA_URL = '/media/'\r\nSTATIC_ROOT = '/var/www/static/'\r\nSTATIC_URL = '/static/'\r\n\r\n# run this whenever static files are changed\r\n$ ./manage.py collectstatic</pre>\r\n<p>In the django settings file there are four settings. MEDIA_ROOT, MEDIA_URL, STATIC_ROOT, and STATIC_URL. The ROOT settings are the local directories where the files are located. The URL settings are the URLs where these files will be served. Assume we have a file named css/main.css Using the above example, the file will be located in /var/www/static/css/main.css after it is collected. It will be served from the URL http://yourdomain.com/static/css/main.css.</p>\r\n<p>One more benefit of the static system is that you can use pip to add static files to your project. For example, you can pip install <a href=\"http://pypi.python.org/pypi/django-staticfiles-jquery\">django-staticfiles-jquery</a> and <a href=\"http://pypi.python.org/pypi/django-staticfiles-bootstrap/2.2.2\">django-staticfiles-bootstrap</a> to automatically put jquery and boostrap into your static files when you run collectstatic.</p>\r\n<h4>Nginx</h4>\r\n<p>This brings us to the final piece of the puzzle. How exactly are these static files served? Gunicorn handles serving all the dynamically generated pages of our application, but it doesn&#8217;t know anything about these static files. The Django runserver will do some magic to handle these files to make development easier, but once you enter production and set DEBUG=False, you no longer have this luxury. That is why we put <a href=\"http://nginx.com/\">Nginx</a> as our web server out in front of Gunicorn. It will serve all the static files, and also ask Gunicorn to handle any requests it doesn&#8217;t know what to do with.</p>\r\n<pre># install nginx\r\n$ sudo apt-get install nginx</pre>\r\n<p>Much like Apache, Nginx uses two folders for configuration. There is /etc/nginx/sites-available/ and /etc/nginx/sites-enabled/. You put all your nginx configuration files into the sites-available directory. Then if you want a site to be active, you create a symlink to that file from the sites-enabled directory. You have to restart nginx after any configuration changes. Because Ubuntu starts nginx with a default configuration when you install it, you must first remove the symlink to that default configuration.</p>\r\n<pre># remove the default symbolic link\r\n$ sudo rm /etc/nginx/sites-enabled/default\r\n\r\n# create a new blank config, and make a symlink to it\r\n$ sudo touch /etc/nginx/sites-available/&lt;YOUR_APP&gt;\r\n$ cd /etc/nginx/sites-enabled\r\n$ sudo ln -s ../sites-available/&lt;YOUR_APP&gt;\r\n\r\n# edit the nginx configuration file\r\n$ vim /etc/nginx/sites-available/&lt;YOUR_APP&gt;</pre>\r\n<p>Here is what the nginx configuration file should look like</p>\r\n<pre># define an upstream server named gunicorn on localhost port 8000\r\nupstream gunicorn {\r\n    server localhost:8000;\r\n}\r\n\r\n# make an nginx server\r\nserver {\r\n    # listen on port 80\r\n    listen 80;\r\n\r\n    # for requests to these domains\r\n    server_name &lt;YOUR_DOMAIN&gt;.com www.&lt;YOUR_DOMAIN&gt;.com;\r\n\r\n    # look in this directory for files to serve\r\n    root /var/www/;\r\n\r\n    # keep logs in these files\r\n    access_log /var/log/nginx/&lt;YOUR_APP&gt;.access.log;\r\n    error_log /var/log/nginx/&lt;YOUR_APP&gt;.error.log;\r\n\r\n    # You need this to allow users to upload large files\r\n    # See http://wiki.nginx.org/HttpCoreModule#client_max_body_size\r\n    # I'm not sure where it goes, so I put it in twice. It works.\r\n    client_max_body_size 0;\r\n\r\n    # THIS IS THE IMPORTANT LINE\r\n    # this tries to serve a static file at the requested url\r\n    # if no static file is found, it passes the url to gunicorn\r\n    try_files $uri @gunicorn;\r\n\r\n    # define rules for gunicorn\r\n    location @gunicorn {\r\n        # repeated just in case\r\n        client_max_body_size 0;\r\n\r\n        # proxy to the gunicorn upstream defined above\r\n        proxy_pass http://gunicorn;\r\n\r\n        # makes sure the URLs don't actually say http://gunicorn \r\n        proxy_redirect off;\r\n\r\n        # If gunicorn takes &gt; 5 minutes to respond, give up\r\n        # Feel free to change the time on this\r\n        proxy_read_timeout 5m;\r\n\r\n        # make sure these HTTP headers are set properly\r\n        proxy_set_header Host            $host;\r\n        proxy_set_header X-Real-IP       $remote_addr;\r\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\r\n    }\r\n}</pre>\r\n<p>Restart nginx and your server is now running and serving your django application perfectly!</p>\r\n<pre># restart nginx\r\n$ sudo service nginx restart</pre>\r\n<h4>Conclusion</h4>\r\n<p>I&#8217;m sure there are going to be errors in this no matter how much I proofread. I also fully admit that there is always more to learn. I welcome any suggested fixes or improvements to any part of this. Just leave a comment below.</p>\r\n<p>I also hope this saves someone some money by letting them run their site on a cheap Linux host instead of a managed host. Most of all I hope it gets some people to view their application as not just the one piece they wrote, but as a whole made up of many separate working parts. And that those other parts are not to be outsourced, but to be embraced. When you control them, you can solve problems with them.</p>", "published": true, "slug": "complete-single-server-django-stack-tutorial"}}, {"pk": 2, "model": "blog.post", "fields": {"description": "https://github.com/bslatkin/dpxdt", "created": "2013-11-12T02:20:46Z", "title": "How to set up and run Depicted (visual, perceptual diff tool) on a mac", "content": "<h4>Introduction</h4>\r\n\r\nWhen it comes to regression testing, manually clicking through your site and scanning webpages for visual changes is an arduous and time consuming task.  Here's how I plan on automating that procedure:\r\n<h4><a href=\"https://github.com/bslatkin/dpxdt\">Depicted-dpxdt</a></h4>\r\nDepicted shows when any visual, perceptual differences are found in webpage screenshots. <a href=\"https://dpxdt-test.appspot.com/\">https://dpxdt-test.appspot.com/</a>\r\n\r\n<h4>Dependencies</h4>\r\n\r\n<ul>\r\n<li>Python 2.7</li>\r\n<li><a href=\"http://brew.sh/\">Homebrew</a></li>\r\n<li><a href=\"https://itunes.apple.com/us/app/xcode/id497799835?mt=12\">Xcode</a></li>\r\n<li><a href=\"http://xquartz.macosforge.org/landing/\">XQuartz</a></li>\r\n<li><a href=\"http://phantomjs.org/\">PhantomJS</a></li>\r\n<li><a href=\"http://www.imagemagick.org/script/index.php\">ImageMagick</a></li>\r\n<li>Ruby</li>\r\n</ul>\r\n\r\n<h3>Getting Started</h3>\r\n<h4>Homebrew</h4>\r\nIf you don't have homebrew installed you will first need to install <a href=\"https://itunes.apple.com/us/app/xcode/id497799835?mt=12\">Xcode</a>.  You will also need to install Xcode's Command Line Tools. After Xcode is installed you can install Homebrew with the following command:\r\n\r\n<pre>\r\nruby -e \"$(curl -fsSL https://raw.github.com/mxcl/homebrew/go)\"\r\n</pre>\r\nIf you already have Homebrew I highly recommend updating:\r\n<pre>\r\nbrew update\r\nbrew upgrade\r\n</pre>\r\n<h4>PhantomJS</h4>\r\ndpxdt wraps PhantomJS's screenshot capabilities.\r\n<pre>\r\nbrew install phantomjs\r\n</pre>\r\n*if you prefer to compile yourself: <a href=\"http://phantomjs.org/download.html\">Download</a>\r\n\r\n<h4>ImageMagick</h4>\r\n<pre>\r\nbrew install imagemagick --with-x11\r\n</pre>\r\nOnce that's installed you may have to add the following export to your .profile or .bash_profile:\r\n<pre>\r\nexport DISPLAY=:\"0\"\r\n</pre>\r\nNext, make sure XQuartz is running then test to see if ImageMagick is working by typing the following in your terminal:\r\n<pre>\r\nconvert logo: logo.gif\r\nidentify logo.gif\r\ndisplay logo.gif\r\n</pre>\r\nOnce ImageMagick is working we can install dpxdt.\r\n<h4>dpxtd</h4>\r\n1. Clone the repo:\r\n<pre>\r\ngit clone https://github.com/bslatkin/dpxdt.git\r\n</pre>\r\n2.  <code>cd</code>  to the repo directory and update all git submodules in the repo:\r\n<pre>\r\n3. git submodule update --init --recursive\r\n</pre>\r\n4. Write a  <code>secrets.py</code>  file to the root directory:\r\n<pre>\r\nSECRET_KEY = 'insert random string of characters here'\r\n</pre>\r\n5. Execute  <code>./run_shell.sh</code>  and run these commands to initialize your DB:\r\n<pre>\r\nserver.db.drop_all()\r\nserver.db.create_all()\r\n</pre>\r\n6. Run the combined server/worker with  <code>./run_combined.sh</code>.<br></br>\r\n7. Navigate to http://localhost:5000.<br></br>\r\n8. Login and create a new build.<br></br>\r\n9. Execute the  <code>./run_url_pair_diff.sh</code>  tool to verify everything is working:\r\n<pre>\r\n./run_url_pair_diff.sh \\\r\n    --upload_build_id=1 \\\r\n    http://www.google.com \\\r\n    http://www.yahoo.com\r\n</pre>\r\nThe above should get you up and running.  For complete instructions, visit <a href=\"https://github.com/bslatkin/dpxdt\">here</a>.", "published": true, "slug": "dpxdt-mac"}}, {"pk": 3, "model": "blog.post", "fields": {"description": "How to scrape a site with Scrapy", "created": "2013-11-14T03:11:55Z", "title": "Creating a powerful web crawler with Scrapy", "content": "<h4>Introduction</h4>\r\n<a href=\"http://doc.scrapy.org/en/0.20/\">Scrapy</a> is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.\r\n<br></br>\r\nI have found a few tutorials but never really found the information I needed/wanted to produce the results that I desired.  I worked with another developer on my team and came up with a simple yet powerful script to scrape a sight and output the results to a file (in this case results being 404s).  Keep in mind, this skips over a large portion of the official documented instructions and in no way represents how a Scrapy project should be run.  A script like this is meant to provide results quickly.\r\n<h4>Installation</h4>\r\n<pre>pip install Scrapy</pre>\r\nI prefer to work with a virtualenv then inside of a directory.  I then place this script inside the directory (I used <a href=\"http://brooklyndelta.com/\">my site</a> as a jump-off):\r\n<h4>Code</h4>\r\n<pre>\r\nfrom urlparse import urlparse\r\nfrom scrapy.selector import HtmlXPathSelector\r\nfrom scrapy.spider import BaseSpider\r\nfrom scrapy.http import Request\r\n\r\nDOMAIN = 'brooklyndelta.com'\r\nURL = 'http://%s' % DOMAIN\r\n\r\nclass BDScrapeSpider(BaseSpider):\r\n    name = \"example\"\r\n    WWW_DOMAIN = 'www.%s' % DOMAIN\r\n    allowed_domains = [DOMAIN, WWW_DOMAIN]\r\n    handle_httpstatus_list = [404]\r\n    start_urls = [\r\n        URL\r\n    ]\r\n\r\n    def parse(self, response):\r\n        hxs = HtmlXPathSelector(response)\r\n        patterns = 'http://', 'https://'\r\n        for url in hxs.select('//a/@href').extract():\r\n            if not url.startswith(patterns):\r\n                if not url.startswith('/'):\r\n                    url = URL + '/' + url\r\n                else:\r\n                    url = URL + url\r\n                    if response.status == 404:\r\n                        print response.url\r\n            yield Request(url, callback=self.parse)\r\n</pre>\r\n<h4>Execution</h4>\r\nYou can run the script using the following command:\r\n<pre>scrapy runspider YOUR_SCRIPT.py</pre>\r\nYou can do a number of things at this point.  If you want, you can port the results to a text file for simple reporting:\r\n<pre>scrapy runspider YOUR_SCRIPT.py > results.txt</pre>\r\nAgain this is a very simple solution to scraping and depending on what site you are scraping you most likely will have to create custom exceptions and parse methods to properly sift through links on the site.  \r\n<h4>Real-world use</h4>\r\nBelow I added conditionals to help scrape <a href=\"http://newyork.craigslist.org/\">craigslist</a> for 404s:\r\n<pre>\r\nfrom urlparse import urlparse\r\nfrom scrapy.selector import HtmlXPathSelector\r\nfrom scrapy.spider import BaseSpider\r\nfrom scrapy.http import Request\r\n\r\nDOMAIN = 'newyork.craigslist.org'\r\nURL = 'http://%s' % DOMAIN\r\n\r\nclass CLScrapeSpider(BaseSpider):\r\n    name = \"example\"\r\n    WWW_DOMAIN = 'www.%s' % DOMAIN\r\n    allowed_domains = [DOMAIN, WWW_DOMAIN]\r\n    handle_httpstatus_list = [404]\r\n    start_urls = [\r\n        URL\r\n    ]\r\n\r\n    def parse(self, response):\r\n        hxs = HtmlXPathSelector(response)\r\n        patterns = 'http://', 'https://'\r\n        for url in hxs.select('//a/@href').extract():\r\n            if not url.startswith(patterns):\r\n                if not url.startswith('/'):\r\n                    url = URL + '/' + url\r\n                else:\r\n                    url = URL + url\r\n                    p = urlparse(response.url)\r\n                    if 'www' in p.path:\r\n                        pass\r\n                    elif 'http://' in p.path:\r\n                        pass\r\n                    elif 'https://' in p.path:\r\n                        pass\r\n                    elif 'mailto' in response.url:\r\n                        pass\r\n                    elif response.status == 404:\r\n                        print response.url\r\n            yield Request(url, callback=self.parse)\r\n</pre>\r\nIn effect the code I am using to parse links also creates bad links.  I think this is just the cost of doing business with this level of automated scraping.  Anyhow, to scrape <a href=\"http://newyork.craigslist.org/\">craigslist</a> I had to create a small waterfall of conditionals so I could get at valid 404s and not ones that I created.\r\n<h4>Conclusion</h4>\r\nI'm sure there are better ways to do this but I just wanted to share perhaps a different perspective on using <a href=\"http://doc.scrapy.org/en/0.20/\">Scrapy</a>.", "published": true, "slug": "web-scraping-scrapy"}}]